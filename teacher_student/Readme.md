# LLM4Gov: Tutorial de Configura√ß√£o e Execu√ß√£o

## Introdu√ß√£o

Grandes modelos de linguagem (LLMs) revolucionaram o processamento de linguagem natural, mas o seu uso no setor p√∫blico pode ser limitado por fatores como custo, necessidade de hardware e restri√ß√µes legais relacionadas √† privacidade. No _paper_ do [**LLM4Gov**](https://sol.sbc.org.br/index.php/wcge/article/view/36338), destacamos que tais modelos costumam ser propriet√°rios, caros e inacess√≠veis para organiza√ß√µes governamentais, especialmente quando os dados cont√™m informa√ß√µes sens√≠veis.

Para contornar esses problemas, foi investigada uma arquitetura de **aprendizado professor‚Äìaluno** (_teacher‚Äëstudent_) com foco em privacidade e baixo custo.  O pipeline √© composto por tr√™s etapas principais: 

1. **Anonimiza√ß√£o** ‚Äì remo√ß√£o de informa√ß√µes pessoais identific√°veis (PII) antes de qualquer intera√ß√£o com um LLM externo.

2. **Gera√ß√£o de instru√ß√µes pelo professor** ‚Äì um modelo de grande porte gera exemplos rotulados a partir do conjunto (amostra) anonimizado.

3. **Ajusto fino do estudante** ‚Äì um LLM destilado √© ajustado usando t√©cnicas eficientes (LoRA e quantiza√ß√£o) para aprender com as instru√ß√µes do professor.

A seguir, √© detalhado como executar o framework do LLM4Gov, incluindo como instalar as depend√™ncias, preparar os dados, executar o modelo **teacher**, realizar o ajuste fino do modelo **student** e realizar infer√™ncia com o modelo treinado. Ao final, tamb√©m √© fornecida uma vers√£o em notebook Jupyter com os mesmos passos.

## Vis√£o geral do Framework LLM4Gov

O **LLM4Gov** foi concebido para preservar a privacidade de documentos sens√≠veis ao longo de todo o fluxo de treinamento. A sequ√™ncia geral √©:

1. **Anonimiza√ß√£o dos dados** ‚Äì Um m√≥dulo de anonimiza√ß√£o  identifica entidades como nomes, endere√ßos, emails e telefones e as substitui por _placeholders_ ([NAME], [ADDRESS], etc.), mantendo a estrutura e o significado das senten√ßas. A princ√≠pio, voc√™ pode usar qualquer m√≥dulo de anonimiza√ß√£o de dados. Tamb√©m pode escolher pular esta etapa, caso n√£o esteja trabalhando com dados sens√≠veis. Se desejar utilizar o conhecer o projeto de anonimiza√ß√£o do LLM4Gov, consulte o m√≥dulo `anonymizer` do reposit√≥rio. Se voc√™ apenas deseja aprender a usar o LLM4Gov, sugerimos usar o dataset `training_unlabeled.json`, que foi gerado de forma sint√©tica para fins did√°ticos.


2. **Gera√ß√£o de instru√ß√µes (Teacher)** ‚Äì O dataset √© passado a um LLM de grande porte (professor). A escolha do modelo professor √© uma decis√£o importante, pois o modelo estudante ir√° aproximar o conhecimento gerado por este professor (incluindo acertos e erros). Esse modelo gera pares ‚ü®instru√ß√£o, entrada, sa√≠da‚ü© que instruem como resolver a tarefa, explicando a decis√£o em formato JSON.  A estrutura de cada exemplo segue o formato **Instruction ‚Üí Input ‚Üí Response**; a sa√≠da cont√©m predi√ß√µes e explica√ß√µes da tarefa.

3. **Ajusto Fino do estudante (Student)** ‚Äì Um LLM destilado √© ajustado a partir dos dados do modelo professor. Voc√™ pode escolher diferentes modelos estudantes pr√©-treinados nesta vers√£o do **LLM4Gov** para realizar o ajuste fino (veja o arquivo `student_fine_tuning_config.yaml`).  O **LLM4Gov** usa LoRA (Low‚ÄëRank Adaptation) para congelar os pesos originais do modelo e adicionar matrizes de baixa dimens√£o para treinamento.  Tamb√©m √© aplicado 4‚Äëbit quantization para reduzir o consumo de mem√≥ria.

4. **Infer√™ncia** ‚Äì Ap√≥s o treinamento, o modelo estudante pode gerar sa√≠das para novos documentos, imitando o comportanto e desempenho do modelo professor. Vale destacar que o modelo estudante tem comportante espec√≠fico para a tarefa. Embora ele possa ter bom desempenho para a tarefa do ajusto fino, o modelo estudante ter√° dificuldades em resolver outras tarefas n√£o ensinadas pelo modelo professor. Por ser um LLM mais compacto, o modelo estudante pode ser executado em hardware mais modesto e, preferencialmente, numa estrutura local e preservando a privacidade.


## Pr√©-requisitos e instala√ß√£o

Siga os passos abaixo em uma m√°quina com Python‚ÄØ‚â•‚ÄØ3.10 e com GPU.

1. **Clone o reposit√≥rio do LLM4Gov** para um diret√≥rio local.

   ```bash
   git clone https://github.com/Labic-ICMC-USP/llm4gov.git
   ```

2. **Crie um ambiente virtual** (opcional, mas recomendado):

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # Linux/macOS
   .\.venv\Scripts\activate  # Windows
   ```
   OBS: o ambiente virtual n√£o precisa ser criado se voc√™ estiver utilizando Google Colab.

3. **Instale as depend√™ncias** listadas em `requirements.txt`:

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

   O arquivo inclui bibliotecas como `unsloth`, `transformers`, `trl`, `langchain` e `openai` necess√°rias para o **LLM4Gov**.

4. **Dados para treinamento e infer√™ncia**: voc√™ precisar√° de um conjunto de documentos para treinamento (`training_unlabeled.json`) que o modelo professor ir√° receber e outro para teste (`test_unlabeled.json`) que o modelo estudante ir√° processar depois de ser treinado.

   Os exemplos devem ter os campos `system_prompt` (string com instru√ß√µes gerais) e `user_prompt` (texto ou JSON com o documento). 

   - **`system_prompt`** ‚Üí sempre uma **string**, define o papel ou o contexto da tarefa.  
   - **`user_prompt`** ‚Üí pode ser **uma string simples** ou **um dicion√°rio estruturado** com os dados de entrada.

   #### Exemplo 1 ‚Äî `user_prompt` como string

    ```json
    [
        {
            "system_prompt": "Voc√™ √© um especialista em an√°lise de contratos.",
            "user_prompt": "O fornecedor n√£o cumpriu o prazo de entrega estabelecido no contrato."
        }
    ]
    ````

    #### Exemplo 2 ‚Äî `user_prompt` como dicion√°rio

    ```json
    [
        {
            "system_prompt": "Voc√™ √© um avaliador de relat√≥rios t√©cnicos.",
            "user_prompt": {
                "id": "DOC-001",
                "text": "A entrega dos equipamentos foi adiada em 10 dias devido a falhas log√≠sticas.",
                "metadata": {
                    "categoria": "infraestrutura",
                    "prioridade": "alta"
                }
            }
        }
    ]
    ```

   Prepare tamb√©m um arquivo `structure_example.json`, definido no `teacher_config.yaml`, que descreve o formato de sa√≠da desejado do professor. Esse arquivo serve como **modelo de estrutura de sa√≠da** que o *Teacher* deve gerar. Na pr√°tic, ele define **como deve ser o formato do JSON** que o LLM precisa retornar, ou seja, as chaves e tipos esperados. Este formato √© livro, definido pela necessidade do proejto, e √© seguido depois tamb√©m pelo modelo estudante. A seguir, um exemplo de estrutura:

    ```json
    {
        "document_id": "IDENTIFICADOR_DO_DOCUMENTO",
        "label": "R√ìTULO_GERADO_PELO_MODELO",
        "explanation": "Breve justificativa da decis√£o ou classifica√ß√£o feita pelo modelo."
    }
    ```

## Gera√ß√£o de instru√ß√µes com o Teacher

Com os dados de trienamento, utilizamos o script `teacher.py` para solicitar a um LLM de grande porte que gere instru√ß√µes e r√≥tulos.

 O **Teacher** implementa um executor com valida√ß√£o de esquema usando `pydantic`, garantindo que as sa√≠das sigam a estrutura definida em `structure_example.json`. Algumas observa√ß√µes:

* O script se baseia na classe `ChatOpenAI` da biblioteca LangChain para conectar‚Äëse em qualquer servi√ßo de LLM via API que suporte este protocolo.

* O arquivo de configura√ß√£o `teacher_config.yaml` permite definir o modelo (por exemplo, `deepseek/deepseek-chat-v3-0324`), a chave da API (`api_key`) e o arquivo de exemplo de estrutura (`schema_example_path`).  Certifique‚Äëse de fornecer uma chave v√°lida de API do servi√ßo (por exemplo, OpenRouter).  Se preferir, defina a vari√°vel de ambiente `OPENAI_API_KEY` em vez de colocar a chave no YAML.

* Os campos `input_json` e `output_json` especificam, respectivamente, o arquivo de dados de entrada e o arquivo de sa√≠da com r√≥tulos.

Para executar o professor:

1. Edite o arquivo `teacher_config.yaml` e ajuste os seguintes itens:
   * `model`: nome do modelo LLM que servir√° como professor (por exemplo, `deepseek/deepseek-chat-v3-0324` ou outro compat√≠vel com a API escolhida).
   * `api_key` e `base_url`: credenciais e URL do servi√ßo de LLM.  Se `api_key` ficar vazia, a chave ser√° lida de `OPENAI_API_KEY`.
   * `input_json`: defina o arquivo anonimizado (por exemplo, `training_unlabeled_anon.json`).
   * `output_json`: nome do arquivo de sa√≠da, onde as instru√ß√µes do professor ser√£o salvas (por padr√£o `training_labeled.json`).
   * `schema_example_path`: caminho para o JSON que descreve o esquema de sa√≠da esperado.

2. Execute o script:

   ```bash
   python teacher.py --config teacher_config.yaml
   ```

   O script carrega cada item, envia ao modelo, valida a sa√≠da JSON e salva no arquivo indicado.  O campo `teacher_output` conter√° o JSON gerado pelo professor.  O log de progresso √© exibido no terminal e gravado no arquivo especificado em `log_file`.

## Ajuste fino do Student (fine‚Äëtuning)

Ap√≥s gerar o conjunto rotulado, treinamos um modelo estudante mais leve.  O script `student_fine_tuning.py` utiliza a biblioteca **Unsloth** para realizar _fine‚Äëtuning_ via LoRA e quantiza√ß√£o.  A abordagem LoRA congela os pesos originais e atualiza apenas matrizes de baixa dimens√£o, reduzindo drasticamente o n√∫mero de par√¢metros a treinar, enquanto a quantiza√ß√£o em 4‚ÄØbits diminui o consumo de mem√≥ria.  

### Configura√ß√£o do fine‚Äëtuning

O arquivo `student_fine_tuning_config.yaml` possui diversas se√ß√µes:

* `model`: define o nome do modelo base (e.g. `unsloth/Phi-3.5-mini-instruct`), o tamanho m√°ximo de sequ√™ncia (`max_seq_length`), o tipo de _dtype_ e se o modelo deve ser carregado em 4‚ÄØbits. Atualmente, o **LLM4Gov** √© funcionado para os **modelos estudantes** abaixo, mas em princ√≠pio qualquer modelo dispon√≠vel no **Unsloth** pode ser utilizado, desde que sejam feitos os devidos ajustes no script de ajuste fino (`student_fine_tuning.py`):

    ```python
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 405B tamb√©m dispon√≠vel em 4bit!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # Novo Mistral 12B, 2x mais r√°pido!
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3, 2x mais r√°pido!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5, 2x mais r√°pido!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit"             # Gemma, 2x mais r√°pido!



* `lora`: par√¢metros do LoRA, como a dimens√£o interna `r`, os m√≥dulos alvo (`target_modules`), `lora_alpha` e `lora_dropout`.
* `train_args`: hiperpar√¢metros de treinamento, incluindo tamanho de lote (`per_device_train_batch_size`), n√∫mero de passos ou √©pocas, taxa de aprendizado e diret√≥rio de sa√≠da.
* `save`: especifica onde salvar os adaptadores LoRA (`save_lora_dir`) e se devem ser gerados modelos combinados (16 ou 4‚ÄØbits) ou arquivos GGUF.
* `data`: indica o arquivo rotulado produzido pelo professor (`labeled_file`) e permite limitar a quantidade de exemplos.

Ajuste esses par√¢metros conforme seus recursos de hardware e tamanho do dataset.  Para um treinamento r√°pido de teste, limite `max_steps` ou `max_examples`.

### Execu√ß√£o do treinamento

1. Verifique se o arquivo rotulado `training_labeled.json` (gerado pelo professor) est√° acess√≠vel no caminho indicado em `student_fine_tuning_config.yaml`.
2. Inicie o treinamento com:

   ```bash
   python student_fine_tuning.py --config student_fine_tuning_config.yaml
   ```

Durante o processo, o script:

* Carrega o modelo base com quantiza√ß√£o de 4‚ÄØbits.
* Anexa as camadas LoRA com as configura√ß√µes especificadas.
* Treina o modelo usando o `SFTTrainer` da biblioteca TRL.
* Salva os adaptadores LoRA e, se configurado, vers√µes fundidas ou quantizadas.

O tempo de treinamento depende do tamanho do conjunto e da GPU. As mensagens de log indicar√£o a utiliza√ß√£o de GPU e o tempo aproximado.

## Infer√™ncia com o Student

Com o modelo estudante treinado, podemos gerar respostas para novos documentos.  O script `student_inference.py` carrega o modelo LoRA e processa exemplos em lote.  As configura√ß√µes est√£o no arquivo `student_inference_config.yaml`:

* `model_path`: diret√≥rio onde os adaptadores LoRA ou modelo fundido foram salvos (por padr√£o, `lora_model`). 
* `max_seq_length`: tamanho m√°ximo de contexto na entrada.
* `max_new_tokens`: limite de tokens que o modelo pode gerar (ajuste conforme a tarefa e a GPU dispon√≠vel).
* `batch_size`: n√∫mero de exemplos processados por vez na infer√™ncia.
* `test_file`: arquivo JSON/JSONL com exemplos a serem processados (deve conter `system_prompt` e `user_prompt`).
* `output_file`: caminho para salvar as respostas geradas pelo estudante.

Para rodar a infer√™ncia:

```bash
python student_inference.py --config student_inference_config.yaml
```

O script realiza os seguintes passos:

* Carrega o modelo LoRA em 4‚ÄØbits.
* Gera as respostas usando decodifica√ß√£o determin√≠stica (sem amostragem).  Ap√≥s a gera√ß√£o, tenta converter a resposta em JSON; se n√£o for poss√≠vel, mant√©m como texto bruto.
* Escreve as sa√≠das em `output_file` com os campos `system_prompt`, `user_prompt` e `student_output`.

## Execu√ß√£o no Google Colab

Este tutorial tamb√©m pode ser executado diretamente no **Google Colab**, inclusive na vers√£o gratuita.  
O ambiente do Colab j√° vem com GPU T4 (‚âà 15 GB VRAM), o que √© **suficiente para realizar o fine-tuning e a infer√™ncia** dos modelos estudantes otimizados com LoRA + quantiza√ß√£o 4 bits.

Para abrir e executar o tutorial online, basta acessar:

üîó [Executar no Google Colab](https://colab.research.google.com/drive/1pge8PjrDpOXxoOzEEO45eaUThscUazqd?usp=sharing)

> Mesmo com recursos limitados, o pipeline do **LLM4Gov** foi projetado para funcionar em GPUs de baixo custo, possibilitando experimenta√ß√£o e reprodutibilidade completa do m√©todo.
