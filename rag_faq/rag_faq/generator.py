import os
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from rag_faq.retriever import retrieve_similar_faqs
from rag_faq.utils import load_prompt_template


def generate_rag_answer(config, project_dir, user_question, debug=False):
    """
    Generate a final answer using retrieved FAQ context and a LLM.
    
    Returns a dictionary with:
      - 'answer': final answer generated by the LLM
      - 'context': list of top-k question-answer entries
      - 'raw_response': full raw string response from LLM (optional)
    """

    # Retrieve top-k similar entries
    top_contexts = retrieve_similar_faqs(config, project_dir, user_question)

    # Build context string for injection
    context_str = ""
    for i, ctx in enumerate(top_contexts):
        context_str += f"[{i+1}]\nText: {ctx['source_text']} \nQuestion: {ctx['question']}\nAnswer: {ctx['answer']}\n\n"

    # Load system prompt for final generation
    prompt_dir = config["paths"]["prompts_dir"]
    system_prompt = load_prompt_template(os.path.join(prompt_dir, "response.txt"))

    # Initialize LLM
    llm_cfg = config["llm"]["rag_answer"]
    model = ChatOpenAI(
        model=llm_cfg["model"],
        temperature=llm_cfg["temperature"],
        openai_api_key=llm_cfg["api_key"],
        base_url="https://openrouter.ai/api/v1" if llm_cfg["provider"] == "openrouter" else None,
    )

    # Call LLM
    response = model.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""Context:\n{context_str}\n\nUser question:\n{user_question}""")
    ])

    # Assemble response object
    result = {
        "answer": response.content.strip(),
        "context": top_contexts,
        "raw_response": response.content  # you can skip this if not needed
    }

    # Optional debug print
    if debug:
        print("\nðŸ”Ž Context used:")
        print(context_str)
        print("\nðŸ“˜ Answer:")
        print(result["answer"])

    return result


def run_rag(config, project_dir):
    """
    Interactive CLI wrapper for RAG querying.
    """
    user_question = input("ðŸ”Ž Enter your question: ")
    result = generate_rag_answer(config, project_dir, user_question, debug=True)
